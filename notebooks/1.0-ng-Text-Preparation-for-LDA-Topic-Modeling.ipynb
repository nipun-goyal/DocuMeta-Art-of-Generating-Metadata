{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preparation for LDA Topic Modeling\n",
    "\n",
    "**Introduction:**\n",
    "<br>The purpose for text pre-processing is to make the text ready for further analysis. Text pre-processing could include many steps depending upon the type of data and the business problem. In our case, we have broken down text pre-processing into three different steps: \n",
    "1. Pre-processing: In the pre-processing, all the text in the corpus was first lowercased so as to avoid algorithm reading **'Environment'** and **'environment'** as two different words. Next, the regular expressions were used to eliminate duplicate whitespaces, remove special characters, numbers and words which were less than three character length followed by the removal of stopwords\n",
    "2. Lemmatization: In the lemmatizer step, words were replaced with their root form using [spaCy](https://spacy.io/api/lemmatizer) lemmatizer. For example: the lemma of the word **'plants'** is **'plant'**. Likewise, **'classifying'** -> **'classify'**. The [NLTK wordnet](https://www.nltk.org/api/nltk.stem.wordnet.html) and [TextBlob](https://textblob.readthedocs.io/en/dev/quickstart.html) lemmatization function has also been provided in this notebook, in case you would like to use these. For this project, we used [spaCy](https://spacy.io/api/lemmatizer) lemmatizer\n",
    "3. Noun extraction: In the noun extraction step, only the nouns were extracted and other parts of speech were ignored as nouns are more indicative of the topic of the document. [spaCy](https://spacy.io/usage/linguistic-features#pos-tagging) Part-of-speech tagging linguistic feature was utilized for noun extraction\n",
    "\n",
    "**Objective:**\n",
    "<br>Making the text ready to be used for topic modeling with only the words of interest\n",
    "\n",
    "**Data Input:** \n",
    "<br>The input for text pre-processing is the corpus of the documents of interest i.e. `.txt` files converted from their pdf version (for eg. ESA documents in our analysis) saved on a local directory\n",
    "\n",
    "**Output:**\n",
    "<br>The output after text pre-processing is a folder in Python's working directory which would have `.txt` files for each pdf document with the resulting noun words in their lemmatized root form\n",
    "\n",
    "**Python Libraries Used:**\n",
    "1. [NLTK](https://www.nltk.org/) for removing stopwords\n",
    "2. [spaCy](https://spacy.io/) for Lemmatization and noun extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import operator\n",
    "import csv\n",
    "import tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.max_length = 6000000 #spaCy cannot process more than 1 million characters at once. Therefore nlp.max.length has to be changed as per the length of the text fed into the spaCy library functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "ROOT_PATH = Path('.').resolve().parents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(mystr):\n",
    "    my_new_str = re.sub(\"(\\\\W| +)\",\" \", mystr) #remove anything that is not a letter or number\n",
    "    my_new_str = re.sub(r'\\s+', ' ', my_new_str) #eliminate duplicate whitespaces\n",
    "    my_new_str = re.sub(r\"\\b\\d+\\b\", \"\", my_new_str)\n",
    "    my_new_str = re.sub(\"\\d+\", \"\", my_new_str) #remove numbers from a string\n",
    "    my_new_str = my_new_str.replace('Ã©', 'e')\n",
    "    my_new_str = re.sub(r\"[^a-zA-Z0-9]+\",' ', my_new_str) #remove special characters\n",
    "    my_new_str = re.sub(r'\\b\\w{1,2}\\b', '', my_new_str) #remove words of length less than 3 from string\n",
    "    my_new_str = re.sub(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*','', my_new_str) #remove stopwords\n",
    "    my_new_str = my_new_str.strip()\n",
    "    my_new_str = re.sub(' +', ' ', my_new_str)\n",
    "    return my_new_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy Lemmatization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmaspacy(my_new_str):\n",
    "    nlp = en_core_web_sm.load()\n",
    "    nlp.max_length = 60000000\n",
    "    sentence = my_new_str\n",
    "    doc = nlp(sentence)\n",
    "    return \" \".join([token.lemma_ for token in doc]) # joining all the word tokens after lemmatizer implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Files and Initializing Functions clean(), lemmaspacy() and Extracting Nouns\n",
    "**Note:** Below code could take very long to run depending on the number of files you have in the corpus. You could either fire this code onto GPU machine or implement `multiprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_documents = []\n",
    "file_names = []\n",
    "text_files_dir = ROOT_PATH / \"data\" / \"processed\" / \"text_files\" #directory path where PDFs as text files are saved\n",
    "for file in os.listdir(text_files_dir):\n",
    "    with codecs.open(str(ROOT_PATH / \"data\" / \"processed\" / \"text_files\") + '/' + file,'r', encoding='utf-8') as corpus: #directory path where PDFs as text files are saved\n",
    "        file_names.append(file) \n",
    "        input_str = corpus.read().lower()\n",
    "        input_str = clean(input_str)\n",
    "        input_str = lemmaspacy(input_str) \n",
    "        list_element= \"\"\n",
    "        input_str = nlp(input_str)\n",
    "        for chunk in input_str.noun_chunks: #for loop for noun extraction\n",
    "            list_element = list_element +\" \"+chunk.text #appending nouns separated by a space\n",
    "        list_documents.append(list_element) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing all the pre-processed, lemmatized and noun extracted .txt files in Python's Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_documents)):\n",
    "    with open(str(ROOT_PATH / \"data\" / \"interim\" / \"Noun_text_files\") + '/' + file_names[i], 'w') as f:\n",
    "        f.write(list_documents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix (other lemmatization approaches that were tried)\n",
    "Code for Wordnet Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmawordnet(my_new_str):\n",
    "    sentence_words = nltk.word_tokenize(my_new_str)\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(word, pos='n') for word in sentence_words]) \n",
    "    return lemmatized_output\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for Textblob Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from textblob import TextBlob, Word\n",
    "def lemmatextblob(my_new_str):\n",
    "    sentence = my_new_str\n",
    "    sent = TextBlob(sentence)\n",
    "    tag_dict = {\"J\": 'a', \n",
    "                \"N\": 'n', \n",
    "                \"V\": 'v', \n",
    "                \"R\": 'r'}\n",
    "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n",
    "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
    "    return \" \".join(lemmatized_list)\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('documeta': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e915cf99c0e1768ae0033093a0e24e3b21951a38403e44c956a008cd194c14d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
